# Processing (Apache Spark)
https://spark-summit.org/2014/training/

## 1. Introducing Apache Spark
- Spark Summit 2014 행사 중 Databricks에서 2014년에 발표한 Apache Spark에 대한 소개자료
- Spark에 대한 기본 개념에서 Streaming, SQL, MLlib, GraphX까지 전반적인 개념파악에 도움
- 강의 slice : http://training.databricks.com/workshop/itas_workshop.pdf
- 강의 영상 : https://youtu.be/VWeWViFCzzg?list=PLTPXxbhUt-YWSgAUhrnkyphnh0oKIT8-j
- 주요 내용

```
The Introduction to Apache Spark workshop is for users to learn the core Spark APIs. This session features hands-on technical exercises to get developers up to speed in using Spark for data exploration, analysis, and building big data applications.

The integrated lecture and lab format covers the following topics:

 - Overview of Big Data and Spark
 - Installing Spark Locally
 - Using Spark’s Core APIs in Scala, Java, & Python
 - Building Spark Applications
 - Deploying on a Big Data Cluster
 - Building Applications for Multiple Platforms
```


## 2. Advanced Apache Spark
- Spark의 핵심 모듈에 대하여 좀 더 세부적으로 설명하는 자료
- 실시간 처리를 위한 streaming 부분과 머신러닝을 위한 MLlib를 참고하면 좋음.
- 실습 자료 : https://databricks-training.s3.amazonaws.com/index.html
- 주요 내용

```
The Advanced Apache Spark Workshop will cover advanced topics on architecture, tuning, and each of Spark’s high-level libraries (including the latest features). Attendees will have the opportunity after the lunch break to work through labs on each of the libraries.

Some familiarity with Spark or MapReduce is expected, as this workshop will not cover basic Spark programming.

Topics covered include:

 - Advanced Spark Internals and Tuning – Reynold Xin – SLIDES | VIDEO
 - Spark SQL – Michael Armburst – SLIDES | VIDEO
 - Spark Streaming – https://databricks-training.s3.amazonaws.com/slides/Spark%20Summit%202014%20-%20Spark%20Streaming.pdf
 - MLlib – https://databricks-training.s3.amazonaws.com/slides/Spark_Summit_MLlib_070214_v2.pdf
 - GraphX – Ankur Dave – SLIDES | VIDEO
```

## 3. Install Apache spark (2)

### Install git 
```
> cd ~
> sudo yum install -y git unzip
> git clone https://github.com/freepsw/RTS_Practice.git
> unzip ~/RTS_Practice/data.zip

# spark에서 경로를 인식할 수 있도록 파일 복사 
> cp -R RTS_Practice/data /tmp/
```

### Install JDK 1.8 
```
> sudo yum install -y java

# 한글인 경우 
> alternatives --display java | grep '현재 /'| sed "s/현재 //" | sed 's|/bin/java로 링크되어 있습니다||'
> export JAVA_HOME=$(alternatives --display java | grep '현재 /'| sed "s/현재 //" | sed 's|/bin/java로 링크되어 있습니다||')

# 영문인 경우
> alternatives --display java | grep current | sed 's/link currently points to //' | sed 's|/bin/java||'
> export JAVA_HOME=$(alternatives --display java | grep current | sed 's/link currently points to //' | sed 's|/bin/java||')

> echo "export JAVA_HOME=$JAVA_HOME" >> ~/.bash_profile
> source ~/.bash_profile
> java -version
openjdk version "1.8.0_302"
OpenJDK Runtime Environment (build 1.8.0_302-b08)
OpenJDK 64-Bit Server VM (build 25.302-b08, mixed mode)
```

### Install python3.6
```
> sudo yum install -y https://repo.ius.io/ius-release-el7.rpm
> sudo yum install -y python36u python36u-libs python36u-devel python36u-pip
> python3 -V
Python 3.6.8
```

### Install Apache spark 3.1.2
```
>  curl -O https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz
>  tar xvf spark-3.1.2-bin-hadoop3.2.tgz
```


## 4. Apache Spark 코드 예제
### 4.1 Run pySpark
```
> cd ~/spark-3.1.2-bin-hadoop3.2
> ./bin/pyspark
아래와 같은 메세지가 출력되면 정상

Python 3.6.8 (default, Nov 16 2020, 16:55:22)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] on linux
Type "help", "copyright", "credits" or "license" for more information.
21/10/08 11:30:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.1.2
      /_/

Using Python version 3.6.8 (default, Nov 16 2020 16:55:22)
Spark context Web UI available at http://spark-test.asia-northeast1-b.c.citric-optics-328310.internal:4040
Spark context available as 'sc' (master = local[*], app id = local-1633692641697).
SparkSession available as 'spark'.
```

### 4.2 Run Spark API (pySpark 실습)
```
# SparkSession 확인하기 
> spark
<pyspark.sql.session.SparkSession object at 0x7f4993ea8b00>

# Transformation 
> myRange = spark.range(1000).toDF("number")

# Action
> myRange.first()
Row(number=0)


# 파일 읽기 
> flightData2015 = spark\
  .read\
  .option("inferSchema", "true")\
  .option("header", "true")\
  .csv("/tmp/data/flight-data/csv/2015-summary.csv")

> flightData2015.take(3)


> flightData2015.sort("count").explain()


# 파티션 제 조정
> spark.conf.set("spark.sql.shuffle.partitions", "5")
> flightData2015.take(2)

```

### 4.3 SparkSQL
```
> flightData2015.createOrReplaceTempView("flight_data_2015")

> maxSql = spark.sql("""
SELECT DEST_COUNTRY_NAME, sum(count) as destination_total
FROM flight_data_2015
GROUP BY DEST_COUNTRY_NAME
ORDER BY sum(count) DESC
LIMIT 5
""")

> maxSql.show()

# pyspark package 활용
from pyspark.sql.functions import desc

flightData2015\
  .groupBy("DEST_COUNTRY_NAME")\
  .sum("count")\
  .withColumnRenamed("sum(count)", "destination_total")\
  .sort(desc("destination_total"))\
  .limit(5)\
  .show()

```

### 4.4 Structured Streaming
```
# 테이블 생성
> staticDataFrame = spark.read.format("csv")\
  .option("header", "true")\
  .option("inferSchema", "true")\
  .load("/tmp/data/retail-data/by-day/*.csv")

staticDataFrame.createOrReplaceTempView("retail_data")
staticSchema = staticDataFrame.schema


# 데이터 집계 (1일 단위)
> from pyspark.sql.functions import window, column, desc, col

> staticDataFrame\
  .selectExpr(
    "CustomerId",
    "(UnitPrice * Quantity) as total_cost",
    "InvoiceDate")\
  .groupBy(
    col("CustomerId"), window(col("InvoiceDate"), "1 day"))\
  .sum("total_cost")\
  .show(5)



# Structured Streaming으로 변경
# readStream으로 파일을 실시간으로 읽어옴
> streamingDataFrame = spark.readStream\
    .schema(staticSchema)\
    .option("maxFilesPerTrigger", 1)\
    .format("csv")\
    .option("header", "true")\
    .load("/tmp/data/retail-data/by-day/*.csv")

> purchaseByCustomerPerHour = streamingDataFrame\
  .selectExpr(
    "CustomerId",
    "(UnitPrice * Quantity) as total_cost",
    "InvoiceDate")\
  .groupBy(
    col("CustomerId"), window(col("InvoiceDate"), "1 day"))\
  .sum("total_cost")

# 읽어온 결과를 실시간으로 메모리에 저장
> purchaseByCustomerPerHour.writeStream\
    .format("memory")\
    .queryName("customer_purchases")\
    .outputMode("complete")\
    .start()

# 메모리에 저장된 결과를 출력
> spark.sql("""
  SELECT *
  FROM customer_purchases
  ORDER BY `sum(total_cost)` DESC
  """)\
  .show(5)

```


### 4.2 spark-submit 
- https://spark.apache.org/docs/latest/submitting-applications.html
```
> cd ~/spark-3.1.2-bin-hadoop3.2
> ./bin/spark-submit   --class org.apache.spark.examples.SparkPi   --master local[2]   /home/freepsw.10/spark/spark-3.1.2-bin-hadoop3.2/examples/jars/spark-examples_2.12-3.1.2.jar 100

# 아래 결과 출력
21/10/08 23:34:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
21/10/08 23:34:35 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 3.096945 s
Pi is roughly 3.141575514157551
```





## 5. 실습 프로젝트
- https://github.com/freepsw/simple-streaming
- kafka -> apache spark(streaming, mllib)-> (redis, elasticsearch) -> (kibana, nodejs)
- 위의 시나리오에 따라 동작하는 프로그램 개발 (작업 중)

### 3.1 install and run spark

#### install apache spark 2.1.1
```
> cd ~/apps
> wget http://d3kbcqa49mib13.cloudfront.net/spark-2.1.1-bin-hadoop2.7.tgz
> tar xvf spark-2.1.1-bin-hadoop2.7.tgz

# spark cluster를 구성할 worker node를 추가한다.
# 실습에서는 1대에 master와 worker를 1개씩 설치하므로, localhost만 추가한다.
> cd ~/apps/spark-2.1.1-bin-hadoop2.7/conf
> cp slaves.template slaves
> vi slaves
localhost
```

#### run spark Cluster
```
> cd ~/apps/spark-2.1.1-bin-hadoop2.7
> sbin/start-all.sh
```
- "http://server-ip:8080" 접속


- Error 발생
 - "localhost: Permission denied (publickey,gssapi-keyex,gssapi-with-mic)."
- 원인
 - worker를 구동하기 위하여 ssh로 접속해야 하는데,
 - 현재 ssh로 접속하기 위한 ssh key가 존재하지 않음
- 해결
```
> ssh-keygen -t rsa -P ""
> cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys
```

### 3.2 setting environments for spark streaming applications
#### clone git repository
- 아래 github에서 제공하는 코드를 사용하여 실습 진행
```
> sudo yum install -y git
> cd apps
> git clone https://github.com/PacktPublishing/Apache-Spark-2-for-Beginners.git
```

#### install sbt compiler
- http://www.scala-sbt.org/release/docs/Installing-sbt-on-Linux.html 참고
```
> curl https://bintray.com/sbt/rpm/rpm | sudo tee /etc/yum.repos.d/bintray-sbt-rpm.repo
> sudo yum install sbt
```

#### 환경변수 등록 (SPARK_HOME)
```
> vi ~/.bash_profile
export SPARK_HOME=/home/freepsw.01/apps/spark-2.1.1-bin-hadoop2.7
> source ~/.bash_profile
```


#### run spark streaming application

### 3.3 run socket event processing example (scala app)

#### source code

```scala
package com.packtpub.sfb

import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.storage.StorageLevel
import org.apache.log4j.{Level, Logger}

object StreamingApps{
  def main(args: Array[String])
  {
	// Log level settings
	LogSettings.setLogLevels()
	// Create the Spark Session and the spark context				
	val spark = SparkSession
			.builder
			.appName(getClass.getSimpleName)
			.getOrCreate()
	// Get the Spark context from the Spark session for creating the streaming context
	val sc = spark.sparkContext
  	// Create the streaming context
	val ssc = new StreamingContext(sc, Seconds(10))
	// Set the check point directory for saving the data to recover when there is a crash
	ssc.checkpoint("/tmp")
	println("Stream processing logic start")
	// Create a DStream that connects to localhost on port 9999
	// The StorageLevel.MEMORY_AND_DISK_SER indicates that the data will be stored in memory and if it overflows, in disk as well
	val appLogLines = ssc.socketTextStream("localhost", 9999, StorageLevel.MEMORY_AND_DISK_SER)
	// Count each log message line containing the word ERROR
	val errorLines = appLogLines.filter(line => line.contains("ERROR"))
	// Print the elements of each RDD generated in this DStream to the console
	errorLines.print()
	// Count the number of messages by the windows and print them
	errorLines.countByWindow(Seconds(30), Seconds(10)).print()
	println("Stream processing logic end")
	// Start the streaming
	ssc.start()   
	// Wait till the application is terminated	          
	ssc.awaitTermination()  
  }
}


object LogSettings{
  def setLogLevels() {
    val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements
    if (!log4jInitialized) {
	    // This is to make sure that the console is clean from other INFO messages printed by Spark
	    Logger.getRootLogger.setLevel(Level.WARN)
    }
  }
}
```

#### compile streaming application
```
> cd ~/apps/Apache-Spark-2-for-Beginners/
> cd Code_Chapter\ 6/Code/Scala
# shell을 실행할 수 있는 권한 설정
> chmod +x *.sh

# config.sbt 수정
> vi config.sbt
val sparkVersion = "2.1.1"  --> SNAPSHOT 제거
> ./compile.sh
```

#### spark application 실행
```
- 0은 kafka를 사용하는지 구분하는 단어 (1인 경우 kafka 적용)
> ./submit.sh com.packtpub.sfb.StreamingApps 0
```

#### netcat 실행
```
> nc -lk 9999
[Wed Oct 11 14:32:52 2000] [ERROR] [client 127.0.0.1] client denied by server configuration: /export/home/live/ap/htdocs/test
```

- 이렇게 실행하면 spark application 로그에 해당 문자가 출력됨
```
-------------------------------------------
Time: 1494145140000 ms
-------------------------------------------
[Wed Oct 11 14:32:52 2000] [ERROR] [client 127.0.0.1] client denied by server configuration: /export/home/live/ap/htdocs/test

-------------------------------------------
Time: 1494145140000 ms
-------------------------------------------
1
```

### 3.4 run socket event processing example (python app)

```
> cd ~/apps/Apache-Spark-2-for-Beginners/Code_Chapter\ 6/Code/Python
# shell을 실행할 수 있는 권한 설정
> chmod +x *.sh
```

#### python
```python
from __future__ import print_function
import sys
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

if __name__ == "__main__":
    # Create the Spark context
    sc = SparkContext(appName="PythonStreamingApp")
    # Necessary log4j logging level settings are done
    log4j = sc._jvm.org.apache.log4j
    log4j.LogManager.getRootLogger().setLevel(log4j.Level.WARN)
    # Create the Spark Streaming Context with 10 seconds batch interval
    ssc = StreamingContext(sc, 10)
    # Set the check point directory for saving the data to recover when there is a crash
    ssc.checkpoint("\tmp")
    # Create a DStream that connects to localhost on port 9999
    appLogLines = ssc.socketTextStream("localhost", 9999)
    # Count each log messge line containing the word ERROR
    errorLines = appLogLines.filter(lambda appLogLine: "ERROR" in appLogLine)
    # // Print the elements of each RDD generated in this DStream to the console
    errorLines.pprint()
    # Count the number of messages by the windows and print them
    errorLines.countByWindow(30,10).pprint()
    # Start the streaming
    ssc.start()
    # Wait till the application is terminated
    ssc.awaitTermination()
```

### run python application
```
>  cd ~/apps/Apache-Spark-2-for-Beginners/Code_Chapter\ 6/Code/Python
> ./submitPy.sh StreamingApps.py
```
