# Collect (Apache Kafka)
## 1. Install & start kafka [link](http://kafka.apache.org/documentation.html#quickstart)

###  Step 1: Download the code
```
> cd ~/apps/
> wget http://apache.mirror.cdnetworks.com/kafka/2.2.0/kafka_2.12-2.2.0.tgz
> tar xvf kafka_2.12-2.2.0.tgz
```

### Step 2: Start Zookeeper server
```
> cd kafka_2.12-2.2.0
> bin/zookeeper-server-start.sh config/zookeeper.properties //zookeeper 시작
```

### Step 2: Start Kafka server
-  kafka delete option 설정
  - topic을 삭제할 수 있는 옵션 추가 (운영서버에서는 이 옵션을 false로 설정. topic을 임의로 삭제할 수 없도록)
```
> cd ~/apps/kafka_2.12-2.2.0
> vi config/server.properties
  # 아래 내용 추가
  delete.topic.enable=true

> bin/kafka-server-start.sh config/server.properties
```


### Step 3: Create a topic
```
> cd ~/apps/kafka_2.12-2.2.0
> bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test

> bin/kafka-topics.sh --list --zookeeper localhost:2181
```

### Step 4: Send some messages
```
> bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
This is a message
This is another message
```

### Step 5: Start a consumer
```
> cd ~/apps/kafka_2.12-2.2.0
> bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning

This is a message
This is another message

```

### Step 6: Describe Topic
```
> bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test
Topic:test      PartitionCount:1        ReplicationFactor:1     Configs:
        Topic: test     Partition: 0    Leader: 0       Replicas: 0     Isr: 0
```

### Steo 7: Offset monitoring
- https://kafka.apache.org/documentation/#basic_ops_consumer_lag
```
# find consumer group list
> bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list
console-consumer-19344

# view offset of group
> bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group console-consumer-19344

TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                     HOST            CLIENT-ID
test            0          2               2               0               consumer-1-ff412a90-fec0-45e4-89d9-06179c7bd8e3 /10.146.0.6     consumer-1
```

### Delete Topic
```
> bin/kafka-topics.sh --delete --zookeeper localhost:2181  --topic streams-file-input
```


## 5. Collect data using apache flume , apache kafka, logstash

### 01. Create Kafka topic
```
> cd ~/apps/kafka_2.12-2.2.0
> bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic mytopic

> bin/kafka-topics.sh --list --zookeeper localhost:2181
```

### 02. Collect logs and send logs to kafka using apache flume
- 아래 링크를 이용하여 Apache flume을 설치 및 실행 테스트
- https://github.com/freepsw/RTS_Practice/blob/master/01.Apache%20Flume.md#1-collectionapache-flume
#### Create Flume config

```
> cd ~/apps/apache-flume-1.8.0-bin/conf
> vi nc-kafka.conf

a1.sources = r1
a1.channels = c1
a1.sinks = s1

a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 56565
a1.sources.r1.channels = c1

a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

a1.sinks.s1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.s1.topic = mytopic
a1.sinks.s1.brokerList = localhost:9092
a1.sinks.s1.requiredAcks = 1
a1.sinks.s1.batchSize = 20
a1.sinks.s1.channel = c1
```

#### run flume agent
```
> cd ~/apps/apache-flume-1.8.0-bin/
> ./bin/flume-ng agent -c ./conf -f ./conf/nc-kafka.conf -name a1 -Dflume.root.logger=INFO,console
```

### 03. Consume Kafka message using logstash

#### Create logstash config
```
> cd ~/apps/logstash-6.4.2
> vi kafka-input.conf

input {
  kafka{
  	topics => ["토픽명"]
  	bootstrap_servers => "kafka브로커ip:9092"
  }
}

output {
  stdout { codec => rubydebug }
}
```
- logstash 실행

### 04. Send Logs to apache flume using necat command
```
> curl telnet://localhost:56565
hi message
OK
```


### 05. Check logstash console
- 아래와 같이 necat으로 보낸 메세지를 받는지 확인한다.
```json
{
       "message" => "hi message",
      "@version" => "1",
    "@timestamp" => 2018-10-12T14:18:36.953Z
}
```

#### 06. KafkaOffsetMonitoring
- Kafka의 주요 topic, consumer group, offset 값을 모니터링 한다.
- https://github.com/Morningstar/kafka-offset-monitor 참고
```
> cd ~/apps/apache-flume-1.8.0-bin/
> wget https://github.com/Morningstar/kafka-offset-monitor/releases/download/0.4.6/KafkaOffsetMonitor-assembly-0.4.6-SNAPSHOT.jar
> java -cp KafkaOffsetMonitor-assembly-0.4.6-SNAPSHOT.jar \
      com.quantifind.kafka.offsetapp.OffsetGetterWeb \
    --offsetStorage kafka \
    --kafkaBrokers localhost:9092 \
    --zk localhost:2181 \
    --port 8081 \
    --refresh 10.seconds \
    --retain 2.days
```

## 10. Use Kafka Connect to import/export data
- 별도의 수집용 code를 만들지 않고, kafka connect를 이용하여 데이터 import 및 export 할 수 있다.
- Secenario : file을 import하고, file로 export 한다.
```
#  creating some seed data to test with
> echo -e "foo\nbar" > test.txt

# start two connectors running in standalone mode
# 3개의 config 파일을 파라미터로 넘긴다.
# 1. Kafka connect process용 config (broker info, data format ...)
# 2. source에 대한 config (test.txt 지정)
# 3. sink에 대한 config (test.sink.txt 지정)
> bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties

# check test.sink.txt file
> cat test.sink.txt
foo
bar

# send another message
> echo "Another line" >> test.txt

# check test.sink.txt file again


# check kafka topic message
> bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic connect-test --from-beginning

```
## 20. Use Kafka Streams to process data
```
# create message and publish to topic "streams-file-input"
> cd $KAFKA_HOME
> echo -e "all streams lead to kafka\nhello kafka streams\njoin kafka summit" > file-input.txt
> cat file-input.txt | ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic streams-file-input

# run kafka stream
> ./bin/kafka-run-class.sh org.apache.kafka.streams.examples.wordcount.WordCountDemo

# check result of kafka stream
> ./bin/kafka-console-consumer.sh --zookeeper localhost:2181 \
            --topic streams-wordcount-output \
            --from-beginning \
            --formatter kafka.tools.DefaultMessageFormatter \
            --property print.key=true \
            --property print.key=true \
            --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \
            --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer

all     1
streams 1
lead    1
to      1
kafka   1
hello   1
kafka   2
streams 2
join    1
kafka   3
summit  1

```
