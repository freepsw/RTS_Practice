# Part 1.  Collect/Simple Processing + Store + visualization

## 1. Logstash (Collect)

### logstash를 이용하여 데이터를 수집하는 Quick guide
 - https://www.elastic.co/guide/en/logstash/current/getting-started-with-logstash.html

- logstash 구성도
![logstash 구조]
(https://www.elastic.co/guide/en/logstash/current/static/images/basic_logstash_pipeline.png)


## 2. elasticsearch

### elasticsearch에 대한 기본 개념 이해
- https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started.html

## 3. kibana
- elasticsearch에 저장된 데이터를 시각화 해주는 web dash-board
 - https://www.elastic.co/guide/en/kibana/current/getting-started.html


# [Part 2]. Apache log 시각화 실습



## 1. Install

### 사전 OS 설정
#### - virtual memory error 해결
  * 시스템의 nmap count를 증가기켜야 한다.
  * https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html
```
# 1) 현재 서버상태에서만 적용하는 방식
> sudo sysctl -w vm.max_map_count=262144

# 2) 영구적으로 적용 (서버 재부팅시 자동 적용)
> sudo vi /etc/sysctl.conf
# 아래 내용 추가
vm.max_map_count = 262144

> sudo sysctl -p
```
#### - File Descriptor 오류 해결
  * file descriptor 갯수를 증가시켜야 한다.
  * max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]
  * https://www.elastic.co/guide/en/elasticsearch/reference/current/setting-system-settings.html#limits.conf
```
> sudo vi /etc/security/limits.conf
# 아래 내용 추가 (rts는 사용자 계정명)
* hard nofile 70000
* soft nofile 70000
root hard nofile 70000
root soft nofile 70000

# 적용을 위해 콘솔을 닫고 다시 연결한다.
# 적용되었는지 확인
> ulimit -a
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 59450
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 70000  #--> 정상적으로 적용됨을 확인함
```

### elasticsearch 6.4 설치 및 실행

#### - 설치
```
> cd ~/apps
> wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.4.2.tar.gz
> tar xvf elasticsearch-6.4.2.tar.gz
> cd elasticsearch-6.4.2

# install plugin
> bin/elasticsearch-plugin install ingest-user-agent
> bin/elasticsearch-plugin install ingest-geoip

# config 설정
> vi config/elasticsearch.yml
# 아래의 코드 추가 (외부 접속 허용)
network.host: 0.0.0.0
```



#### - 실행
```
> ~/apps/elasticsearch-6.4.2/bin/elasticsearch

//정상동작 확인 (Web browser에서 아래 주소 입력하면 결과 json 확인)
http://<ip>:9200

{
"name": "Norrin Radd",
"cluster_name": "elasticsearch",
"version": {
"number": "2.4.0",
"build_hash": "ce9f0c7394dee074091dd1bc4e9469251181fc55",
"build_timestamp": "2016-08-29T09:14:17Z",
"build_snapshot": false,
"lucene_version": "5.5.2"
},
"tagline": "You Know, for Search"
}
```

### kibana 6.4 설치 및 실행
```
> wget https://artifacts.elastic.co/downloads/kibana/kibana-6.4.2-linux-x86_64.tar.gz
> tar xvf kibana-6.4.2-linux-x86_64.tar.gz
> cd kibana-6.4.2-linux-x86_64

# 외부 접속 허용을 위한 설정
> vi config/kibana.yml
server.host: "0.0.0.0"

> bin/kibana
```

### logstash 설치 및 실행

- 사전 설정
```
# java 설치
> java -version
```

- 설치
```
> wget https://artifacts.elastic.co/downloads/logstash/logstash-6.2.4.tar.gz
> tar xvf logstash-6.2.4.tar.gz
> cd logstash-6.2.4
```

- logstash의 정상동작 확인

```
> bin/logstash -e 'input { stdin { } } output { stdout {} }'

// 정상동작 확인을 위한 메세지 입력 후 엔터
checking logstash

// 아래와 같은 메세지가 나오면 정상
[2018-05-02T20:01:49,354][INFO ][logstash.agent           ] Pipelines running {:count=>1, :pipelines=>["main"]}
test
{
      "@version" => "1",
       "message" => "test",
          "host" => "dev02",
    "@timestamp" => 2018-05-02T11:02:06.266Z
}

//Ctrl + D로 종료
```


## 2. logstash를 이용하여 apache log를 elasticsearch에 저장

- 저장할 log 데이터 생성

```
> cd ~
> mkdir logs
> cd logs
> wget https://raw.githubusercontent.com/elastic/examples/master/Common%20Data%20Formats/apache_logs/apache_logs


# apache log 확인
> head apache_logs
//아래와 같은 구조로 생성된 apache log 확인
83.149.9.216 - - [17/May/2015:10:05:03 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-search.png HTTP/1.1" 200 203023 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
```

- logstash 환경설정

```
> wget https://raw.githubusercontent.com/elastic/examples/master/Common%20Data%20Formats/apache_logs/logstash/apache_template.json
# template 버전이 6.0으로 작성되어 _all keywords가 disable되었음
# 따라서 해당 keywords를 삭제한다.
"_all": {
   "enabled": true
}

> wget https://raw.githubusercontent.com/elastic/examples/master/Common%20Data%20Formats/apache_logs/logstash/apache_logstash.conf
> vi apache_logstash.conf

input {  
  stdin { }
}


filter {
  grok {
    match => {
      "message" => '%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}'
    }
  }

  date {
    match => [ "timestamp", "dd/MMM/YYYY:HH:mm:ss Z" ]
    locale => en
  }

  geoip {
    source => "clientip"
  }

  useragent {
    source => "agent"
    target => "useragent"
  }
}

output {
  stdout {
    codec => dots {}
  }

  elasticsearch {
    index => "apache_elastic_example"
    template => "./apache_template.json"
    template_name => "apache_elastic_example"
    template_overwrite => true
  }
}
```

- 실제 환경설정 정보
 - input은 표준입력으로 받음 (이번 실습에서는 cat으로 읽은 파일을 |으로 연결하여 입력)
 - filter는 apache log를 파싱하는데 필요한 설정(elasticsearch에 저장할 필드명과 변수 매핑)
 - output은 elasticsearch의 서버정보, index명, index 생성에 필요한 templagee 등을 설정 (이번 실습에서 index는 "apache_elk_example", 나중에 kibana에서 이 index명으로 시각화)

- logstash 실행

```
> cd ~/logs
> cat apache_logs | ~/logstash-6.2.4/bin/logstash -f apache_logstash.conf
```

- 결과 확인 (웹브라우저)) -> 10,000 건의 apache log를 정상적으로 저장
http://localhost:9200/apache_elastic_example/_count
```
{
"count": 10000,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  }
}
```

## 3. kibana로 시각화 하기.

- kibana 접속 (http://<ip>:5601/)
- elasticsearch에 저장된 index를 조회 (logstash에서 자동으로 생성했음.)
 -  "Management >> Index Patterns >> Index name or pattern" 여기에 index 명을 입력 (apache_elastic_example)
 - 그럼 아래의 create 버튼이 보임 -> 클릭. (timestamp는 logstash에서 설정했으므로 나타남. timestamp를 설정하지 않으면 kibana에서 정상적으로 확인이 어려움.)
 - index의 필드명을 화인할 수 있다.

- 시각화 해보기.
 -  "Management >> Saved Objects >> Import", and select apache_kibana.json
 - https://github.com/elastic/examples/blob/master/Common%20Data%20Formats/apache_logs/logstash/apache_kibana.json
 - 이 json은 사용자가 만든 dash-board를 json형식으로 미리 저장한 파일임.
 - 이 파일을 import하면 리스트에 dash-board명이 나타남.
 - dash-board는 1개이고, 이를 구성하는 visualization은 10개
 - 리스트에 dash-board명을 더블클릭하면 상세 json구조가 보이고,
 - 화면 상위에 "View dashboard" 버튼을 클릭하면 시가화된 데이터가 보인다.


# [Part 3]. 실시간 Twitter trend 시각화 실습
## 1. Install
- Part 2에서 설치한 가이드와 동일

## 2. logstash를 이용하여 twitter 데이터 수집 및 elasticsearch에 저장

- 작업공간으로 이동
```
cd /home/rts/elk/examples/ElasticStack_twitter
```

- logstash 환경설정

```
vi twitter_logstash.conf

// 아래 항목에 twitter에 등록한 key 정보를 입력
input {
  twitter {
    consumer_key       => "INSERT YOUR CONSUMER KEY"
    consumer_secret    => "INSERT YOUR CONSUMER SECRET"
    oauth_token        => "INSERT YOUR ACCESS TOKEN"
    oauth_token_secret => "INSERT YOUR ACCESS TOKEN SECRET"
    keywords           => [ "thor", "spiderman", "wolverine", "ironman", "hulk"]
    full_tweet         => true
  }
}
```
https://github.com/elastic/examples/blob/master/ElasticStack_twitter/twitter_logstash.conf 참고

- logstash 실행

```
~/elk/logstash-2.4.0/bin/logstash -f twitter_logstash.conf
// 아래 메세지가 출력되면 정상
Settings: Default pipeline workers: 1
Pipeline main started
........
```

- 정상적으로 elasticsearch에 저장되는지 확인
http://14.63.218.130:9200/twitter_elk_example/_count
```
{
"count": 43,  // count가 점차 증가함을 볼 수 있다.
  "_shards": {
    "total": 1,
    "successful": 1,
    "failed": 0
  }
}
```

## 3. Kibana로 시각화 하기
- kibana 접속 (http://14.63.218.130:5601/)
- elasticsearch에 저장된 index를 조회 (logstash에서 자동으로 생성했음.)
 -  "Settings tab >> Indices tab >> Index name or pattern" 여기에 index 명을 입력 (twitter_elk_example)
 - 그럼 아래의 create 버튼이 보임 -> 클릭. (timestamp는 logstash에서 설정했으므로 나타남. timestamp를 설정하지 않으면 kibana에서 정상적으로 확인이 어려움.)
 - index의 필드명을 화인할 수 있다.

- 시각화 해보기.
 -  "Settings tab >> Objects tab >> Import", and select twitter_kibana.json
 - 이 json은 사용자가 만든 dash-board를 json형식으로 미리 저장한 파일임.
 - 이 파일을 import하면 리스트에 dash-board명이 나타남.
 - dash-board는 1개이고, 이를 구성하는 visualization은 10개
 - 리스트에 dash-board명을 더블클릭하면 상세 json구조가 보이고,
 - 화면 상위에 "View dashboard" 버튼을 클릭하면 시가화된 데이터가 보인다.


# [Part 4]. FileBeat를 이용한 수집



# [Part 5] Rally를 이용한 성능 측정
## Prerequisite

```
# python3.x 설치
> sudo yum install -y https://centos7.iuscommunity.org/ius-release.rpm
> sudo yum install -y gcc python36.x86_64 python36-devel.x86_64 python36-pip.noarch

# pip3
> sudo yum install -y wget
> wget https://bootstrap.pypa.io/get-pip.py
> sudo python3.6 get-pip.py

# git (1.9 version 이상 설치)
> sudo yum install -y http://opensource.wandisco.com/centos/7/git/x86_64/wandisco-git-release-7-2.noarch.rpm
> sudo yum install -y git
> git --version
```

## Install Rally
- https://esrally.readthedocs.io/en/latest/tournament.html
```
> sudo pip3 install esrally
> esrally configure
> esrally list tracks
```

### configure
- 메트릭 정보를 저장할 장소를 변경한다.
- 기본은 memory로 지정되며, 이를 kibana로 시각화 하려면 elasticsearch로 변경
```
> cd ~/.rally/
> vi rally.ini
```

### Run Rally
```
> esrally --track=http_logs --target-hosts=localhost:9200 --pipeline=benchmark-only
아래와 같은 메시지가 출력되면서, 테스트 진행
INFO] Decompressing track data from [/home/freepsw_02/.rally/benchmarks/data/http_logs/documents-241998.json.bz2] to [/home/freepsw_02/.rally/benchmarks/data/http_logs/documents-241998.json] (resulting size: 22.87 GB) ... [OK]
[INFO] Preparing file offset table for [/home/freepsw_02/.rally/benchmarks/data/http_logs/documents-241998.json] ... [OK]
Running delete-index                                                           [100% done]
Running create-index                                                           [100% done]
Running check-cluster-health                                                   [100% done]
Running index-append                                                           [ 64% done]
```

#### 생성된 index 사이즈 확인
```
GET /_cat/indices?v

health status index       uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   logs-231998 clTa4LI6RsO3MHumgREUdA   5   0   11961342            0    963.7mb        963.7mb
green  open   logs-241998 79Cemjr3TWWNLUdjNBR5Lg   5   0  181463624            0     13.9gb         13.9gb
green  open   logs-211998 dXpoIBXLSVm1bz_rIpgtrQ   5   0   17647279            0      1.3gb          1.3gb
green  open   logs-201998 GHWzwWLGQseoYzINVv4BMA   5   0   13053463            0        1gb            1gb
green  open   logs-221998 pDITaHSTR9WjuHVgawkWXw   5   0   10716760            0    862.7mb        862.7mb
green  open   logs-191998 HQO6kbWWRUirQKAS1bWVBg   5   0    9697882            0    773.2mb        773.2mb
green  open   logs-181998 5wwI1Nr0TTaaRa4yhcsdMQ   5   0    2708746            0    219.3mb        219.3mb

```


### Check Benchmark summary
#### 1) 환경설정 (Default 설정)
- java heap size : 1g
```
|   Lap |                               Metric |         Task |      Value |    Unit |
|------:|-------------------------------------:|-------------:|-----------:|--------:|
|   All |                  Total indexing time |              |     189.92 |     min |
|   All |          Min indexing time per shard |              | 0.00111667 |     min |
|   All |       Median indexing time per shard |              |    1.66316 |     min |
|   All |          Max indexing time per shard |              |    29.9854 |     min |
|   All |                     Total merge time |              |    178.301 |     min |
|   All |             Min merge time per shard |              |          0 |     min |
|   All |          Median merge time per shard |              |   0.572283 |     min |
|   All |             Max merge time per shard |              |    33.2978 |     min |
|   All |            Total merge throttle time |              |    91.9393 |     min |
|   All |    Min merge throttle time per shard |              |          0 |     min |
|   All | Median merge throttle time per shard |              |  0.0506083 |     min |
|   All |    Max merge throttle time per shard |              |    18.3808 |     min |
|   All |                   Total refresh time |              |    44.9106 |     min |
|   All |           Min refresh time per shard |              | 0.00223333 |     min |
|   All |        Median refresh time per shard |              |     0.3881 |     min |
|   All |           Max refresh time per shard |              |    6.88817 |     min |
|   All |                     Total flush time |              |    2.35718 |     min |
|   All |             Min flush time per shard |              |    0.00035 |     min |
|   All |          Median flush time per shard |              | 0.00336667 |     min |
|   All |             Max flush time per shard |              |     0.5248 |     min |
|   All |                   Total Young Gen GC |              |    584.708 |       s |
|   All |                     Total Old Gen GC |              |     72.804 |       s |
|   All |                           Store size |              |    19.0441 |      GB |
|   All |                        Translog size |              |    14.6954 |      GB |
|   All |               Heap used for segments |              |    81.1885 |      MB |
|   All |             Heap used for doc values |              |   0.138538 |      MB |
|   All |                  Heap used for terms |              |    68.3734 |      MB |
|   All |                  Heap used for norms |              |  0.0333252 |      MB |
|   All |                 Heap used for points |              |    5.65936 |      MB |
|   All |          Heap used for stored fields |              |    6.98383 |      MB |
|   All |                        Segment count |              |        547 |         |
|   All |                       Min Throughput | index-append |    42864.4 |  docs/s |
|   All |                    Median Throughput | index-append |    45159.7 |  docs/s |
|   All |                       Max Throughput | index-append |    46552.9 |  docs/s |
|   All |              50th percentile latency | index-append |    820.652 |      ms |
|   All |              90th percentile latency | index-append |    1197.61 |      ms |
|   All |              99th percentile latency | index-append |    2156.19 |      ms |
|   All |            99.9th percentile latency | index-append |     3190.3 |      ms |
|   All |           99.99th percentile latency | index-append |     4584.1 |      ms |
|   All |             100th percentile latency | index-append |    4680.28 |      ms |
|   All |         50th percentile service time | index-append |    820.652 |      ms |
|   All |         90th percentile service time | index-append |    1197.61 |      ms |
|   All |         99th percentile service time | index-append |    2156.19 |      ms |
|   All |       99.9th percentile service time | index-append |     3190.3 |      ms |
|   All |      99.99th percentile service time | index-append |     4584.1 |      ms |
|   All |        100th percentile service time | index-append |    4680.28 |      ms |
|   All |                           error rate | index-append |          0 |       % |
|   All |                       Min Throughput |      default |       5.16 |   ops/s |
|   All |                    Median Throughput |      default |       5.21 |   ops/s |
|   All |                       Max Throughput |      default |       5.26 |   ops/s |
|   All |              50th percentile latency |      default |    36831.5 |      ms |
|   All |              90th percentile latency |      default |    38662.1 |      ms |
|   All |              99th percentile latency |      default |    39053.7 |      ms |
|   All |             100th percentile latency |      default |    39107.1 |      ms |
|   All |         50th percentile service time |      default |     169.71 |      ms |
|   All |         90th percentile service time |      default |    178.218 |      ms |
|   All |         99th percentile service time |      default |    187.883 |      ms |
|   All |        100th percentile service time |      default |    195.832 |      ms |
|   All |                           error rate |      default |          0 |       % |
|   All |                       Min Throughput |         term |      26.12 |   ops/s |
|   All |                    Median Throughput |         term |      26.12 |   ops/s |
|   All |                       Max Throughput |         term |      26.12 |   ops/s |
|   All |              50th percentile latency |         term |    10047.6 |      ms |
|   All |              90th percentile latency |         term |    10207.1 |      ms |
|   All |              99th percentile latency |         term |    10249.6 |      ms |
|   All |             100th percentile latency |         term |    10255.1 |      ms |
|   All |         50th percentile service time |         term |     15.303 |      ms |
|   All |         90th percentile service time |         term |    16.4165 |      ms |
|   All |         99th percentile service time |         term |    25.4912 |      ms |
|   All |        100th percentile service time |         term |    25.5634 |      ms |
|   All |                           error rate |         term |          0 |       % |
|   All |                       Min Throughput |        range |       0.73 |   ops/s |
|   All |                    Median Throughput |        range |       0.73 |   ops/s |
|   All |                       Max Throughput |        range |       0.74 |   ops/s |
|   All |              50th percentile latency |        range |     105733 |      ms |
|   All |              90th percentile latency |        range |     131742 |      ms |
|   All |              99th percentile latency |        range |     137564 |      ms |
|   All |             100th percentile latency |        range |     138195 |      ms |
|   All |         50th percentile service time |        range |    1321.63 |      ms |
|   All |         90th percentile service time |        range |    1415.48 |      ms |
|   All |         99th percentile service time |        range |    1486.73 |      ms |
|   All |        100th percentile service time |        range |    1514.99 |      ms |
|   All |                           error rate |        range |          0 |       % |
|   All |                       Min Throughput |   hourly_agg |       0.19 |   ops/s |
|   All |                    Median Throughput |   hourly_agg |       0.19 |   ops/s |
|   All |                       Max Throughput |   hourly_agg |       0.19 |   ops/s |
|   All |              50th percentile latency |   hourly_agg |    59814.4 |      ms |
|   All |              90th percentile latency |   hourly_agg |      71797 |      ms |
|   All |              99th percentile latency |   hourly_agg |    74170.4 |      ms |
|   All |             100th percentile latency |   hourly_agg |    74640.4 |      ms |
|   All |         50th percentile service time |   hourly_agg |    5304.58 |      ms |
|   All |         90th percentile service time |   hourly_agg |    5488.42 |      ms |
|   All |         99th percentile service time |   hourly_agg |    5636.58 |      ms |
|   All |        100th percentile service time |   hourly_agg |    6129.61 |      ms |
|   All |                           error rate |   hourly_agg |          0 |       % |
|   All |                       Min Throughput |       scroll |      18.19 | pages/s |
|   All |                    Median Throughput |       scroll |      18.37 | pages/s |
|   All |                       Max Throughput |       scroll |      18.42 | pages/s |
|   All |              50th percentile latency |       scroll |    73098.5 |      ms |
|   All |              90th percentile latency |       scroll |     101527 |      ms |
|   All |              99th percentile latency |       scroll |     107692 |      ms |
|   All |             100th percentile latency |       scroll |     108346 |      ms |
|   All |         50th percentile service time |       scroll |     1347.2 |      ms |
|   All |         90th percentile service time |       scroll |    1391.85 |      ms |
|   All |         99th percentile service time |       scroll |    1452.59 |      ms |
|   All |        100th percentile service time |       scroll |    1475.89 |      ms |
|   All |                           error rate |       scroll |          0 |       % |


----------------------------------
[INFO] SUCCESS (took 9126 seconds)
----------------------------------
```


#### 2) java heap size 변경
```
sudo swapoff -a
sysctl -w vm.swappiness=1
# elasticsearch.yml
bootstrap.memory_lock: true

# jvm.options  (4G로 )
4G
```

```
|   Lap |                               Metric |         Task |     Value |    Unit |
|------:|-------------------------------------:|-------------:|----------:|--------:|
|   All |                  Total indexing time |              |   196.145 |     min |
|   All |          Min indexing time per shard |              |         0 |     min |
|   All |       Median indexing time per shard |              |   1.75079 |     min |
|   All |          Max indexing time per shard |              |   30.9184 |     min |
|   All |                     Total merge time |              |   176.269 |     min |
|   All |             Min merge time per shard |              |         0 |     min |
|   All |          Median merge time per shard |              |  0.514025 |     min |
|   All |             Max merge time per shard |              |   32.5764 |     min |
|   All |            Total merge throttle time |              |   92.1618 |     min |
|   All |    Min merge throttle time per shard |              |         0 |     min |
|   All | Median merge throttle time per shard |              | 0.0492333 |     min |
|   All |    Max merge throttle time per shard |              |   18.6032 |     min |
|   All |                   Total refresh time |              |   44.8848 |     min |
|   All |           Min refresh time per shard |              |         0 |     min |
|   All |        Median refresh time per shard |              |   0.39755 |     min |
|   All |           Max refresh time per shard |              |    6.8916 |     min |
|   All |                     Total flush time |              |   2.13355 |     min |
|   All |             Min flush time per shard |              |         0 |     min |
|   All |          Median flush time per shard |              |  0.003725 |     min |
|   All |             Max flush time per shard |              |    0.4184 |     min |
|   All |                   Total Young Gen GC |              |   691.619 |       s |
|   All |                     Total Old Gen GC |              |     7.869 |       s |
|   All |                           Store size |              |   19.0494 |      GB |
|   All |                        Translog size |              |    14.693 |      GB |
|   All |               Heap used for segments |              |   81.0851 |      MB |
|   All |             Heap used for doc values |              |  0.117699 |      MB |
|   All |                  Heap used for terms |              |   68.2905 |      MB |
|   All |                  Heap used for norms |              | 0.0333862 |      MB |
|   All |                 Heap used for points |              |    5.6648 |      MB |
|   All |          Heap used for stored fields |              |   6.97878 |      MB |
|   All |                        Segment count |              |       548 |         |
|   All |                       Min Throughput | index-append |   42591.7 |  docs/s |
|   All |                    Median Throughput | index-append |   44871.4 |  docs/s |
|   All |                       Max Throughput | index-append |     45901 |  docs/s |
|   All |              50th percentile latency | index-append |   828.784 |      ms |
|   All |              90th percentile latency | index-append |   1175.07 |      ms |
|   All |              99th percentile latency | index-append |   2108.44 |      ms |
|   All |            99.9th percentile latency | index-append |   3948.84 |      ms |
|   All |           99.99th percentile latency | index-append |   6671.88 |      ms |
|   All |             100th percentile latency | index-append |   7245.65 |      ms |
|   All |         50th percentile service time | index-append |   828.784 |      ms |
|   All |         90th percentile service time | index-append |   1175.07 |      ms |
|   All |         99th percentile service time | index-append |   2108.44 |      ms |
|   All |       99.9th percentile service time | index-append |   3948.84 |      ms |
|   All |      99.99th percentile service time | index-append |   6671.88 |      ms |
|   All |        100th percentile service time | index-append |   7245.65 |      ms |
|   All |                           error rate | index-append |         0 |       % |
|   All |                       Min Throughput |      default |      1.78 |   ops/s |
|   All |                    Median Throughput |      default |      1.79 |   ops/s |
|   All |                       Max Throughput |      default |       1.8 |   ops/s |
|   All |              50th percentile latency |      default |    238897 |      ms |
|   All |              90th percentile latency |      default |    254002 |      ms |
|   All |              99th percentile latency |      default |    257403 |      ms |
|   All |             100th percentile latency |      default |    257816 |      ms |
|   All |         50th percentile service time |      default |   508.686 |      ms |
|   All |         90th percentile service time |      default |   537.115 |      ms |
|   All |         99th percentile service time |      default |   591.559 |      ms |
|   All |        100th percentile service time |      default |   602.732 |      ms |
|   All |                           error rate |      default |         0 |       % |
|   All |                       Min Throughput |         term |      19.8 |   ops/s |
|   All |                    Median Throughput |         term |     20.23 |   ops/s |
|   All |                       Max Throughput |         term |     20.65 |   ops/s |
|   All |              50th percentile latency |         term |   15860.9 |      ms |
|   All |              90th percentile latency |         term |   15985.3 |      ms |
|   All |              99th percentile latency |         term |   16007.9 |      ms |
|   All |             100th percentile latency |         term |   16009.7 |      ms |
|   All |         50th percentile service time |         term |   22.1654 |      ms |
|   All |         90th percentile service time |         term |   23.9616 |      ms |
|   All |         99th percentile service time |         term |   40.0451 |      ms |
|   All |        100th percentile service time |         term |   48.2151 |      ms |
|   All |                           error rate |         term |         0 |       % |
|   All |                       Min Throughput |        range |       0.7 |   ops/s |
|   All |                    Median Throughput |        range |       0.7 |   ops/s |
|   All |                       Max Throughput |        range |      0.71 |   ops/s |
|   All |              50th percentile latency |        range |    114731 |      ms |
|   All |              90th percentile latency |        range |    143671 |      ms |
|   All |              99th percentile latency |        range |    150026 |      ms |
|   All |             100th percentile latency |        range |    150754 |      ms |
|   All |         50th percentile service time |        range |   1393.75 |      ms |
|   All |         90th percentile service time |        range |   1477.01 |      ms |
|   All |         99th percentile service time |        range |   1515.16 |      ms |
|   All |        100th percentile service time |        range |   1572.43 |      ms |
|   All |                           error rate |        range |         0 |       % |
|   All |                       Min Throughput |   hourly_agg |      0.18 |   ops/s |
|   All |                    Median Throughput |   hourly_agg |      0.18 |   ops/s |
|   All |                       Max Throughput |   hourly_agg |      0.18 |   ops/s |
|   All |              50th percentile latency |   hourly_agg |   84698.6 |      ms |
|   All |              90th percentile latency |   hourly_agg |    103628 |      ms |
|   All |              99th percentile latency |   hourly_agg |    107694 |      ms |
|   All |             100th percentile latency |   hourly_agg |    108153 |      ms |
|   All |         50th percentile service time |   hourly_agg |   5486.85 |      ms |
|   All |         90th percentile service time |   hourly_agg |   5648.42 |      ms |
|   All |         99th percentile service time |   hourly_agg |   5848.49 |      ms |
|   All |        100th percentile service time |   hourly_agg |   5854.29 |      ms |
|   All |                           error rate |   hourly_agg |         0 |       % |
|   All |                       Min Throughput |       scroll |     18.61 | pages/s |
|   All |                    Median Throughput |       scroll |     18.67 | pages/s |
|   All |                       Max Throughput |       scroll |     18.68 | pages/s |
|   All |              50th percentile latency |       scroll |   69037.6 |      ms |
|   All |              90th percentile latency |       scroll |     96096 |      ms |
|   All |              99th percentile latency |       scroll |    102638 |      ms |
|   All |             100th percentile latency |       scroll |    103212 |      ms |
|   All |         50th percentile service time |       scroll |   1338.21 |      ms |
|   All |         90th percentile service time |       scroll |   1376.92 |      ms |
|   All |         99th percentile service time |       scroll |   1459.92 |      ms |
|   All |        100th percentile service time |       scroll |   1542.93 |      ms |
|   All |                           error rate |       scroll |         0 |       % |


----------------------------------
[INFO] SUCCESS (took 8025 seconds)
----------------------------------
```

#### 2) java heap size 변경(8G)

참고
https://nakjunizm.github.io/2017/09/07/ElasticSearch_Heap_Memory/




###### 9) benchmark test 결과 비교 (Java Heap 1G vs 8G)
- java heap_size를 늘리면,
- indexing throughtput이 낮아짐  (초당 indexing 처라량이 낮아짐.)
|                       Min Throughput | index-append |    42864.4 |     40291.8 | -2572.52 |  docs/s |
|                    Median Throughput | index-append |    45159.7 |     43237.5 | -1922.15 |  docs/s |
|                       Max Throughput | index-append |    46552.9 |     44298.6 | -2254.29 |  docs/s |
|              50th percentile latency | index-append |    820.652 |     859.372 |    38.72 |      ms |
- latency도 더 늦어지는 결과가 생김..
|              90th percentile latency |       scroll |     101527 |      110553 |  9025.89 |      ms |
|              99th percentile latency |       scroll |     107692 |      117077 |  9384.82 |      ms |
- GC의 경우 old_gc 건수가 많이 줄어듬
-
```
> esrally list races
Race Timestamp    Track      Track Parameters    Challenge            Car       User Tags
----------------  ---------  ------------------  -------------------  --------  -----------
20181029T050151Z  http_logs                      append-no-conflicts  external
20181029T024207Z  http_logs                      append-no-conflicts  external
20181028T234358Z  http_logs                      append-no-conflicts  external

> esrally compare --baseline=20181028T234358Z --contender=20181029T050151Z

Comparing baseline
  Race timestamp: 2018-10-28 23:43:58
  Challenge: append-no-conflicts
  Car: external

with contender
  Race timestamp: 2018-10-29 05:01:51
  Challenge: append-no-conflicts
  Car: external
|                               Metric |         Task |   Baseline |   Contender |     Diff |    Unit |
|-------------------------------------:|-------------:|-----------:|------------:|---------:|--------:|
|                  Total indexing time |              |     189.92 |     205.337 |  15.4165 |     min |
|          Min indexing time per shard |              | 0.00111667 |           0 | -0.00112 |     min |
|       Median indexing time per shard |              |    1.66316 |     1.81336 |   0.1502 |     min |
|          Max indexing time per shard |              |    29.9854 |     32.5886 |  2.60318 |     min |
|                     Total merge time |              |    178.301 |     194.581 |    16.28 |     min |
|             Min merge time per shard |              |          0 |           0 |        0 |     min |
|          Median merge time per shard |              |   0.572283 |    0.566508 | -0.00577 |     min |
|             Max merge time per shard |              |    33.2978 |     36.3164 |   3.0186 |     min |
|            Total merge throttle time |              |    91.9393 |     96.4666 |  4.52725 |     min |
|    Min merge throttle time per shard |              |          0 |           0 |        0 |     min |
| Median merge throttle time per shard |              |  0.0506083 |   0.0536833 |  0.00308 |     min |
|    Max merge throttle time per shard |              |    18.3808 |     19.3138 |  0.93293 |     min |
|                   Total refresh time |              |    44.9106 |     48.5383 |   3.6277 |     min |
|           Min refresh time per shard |              | 0.00223333 |           0 | -0.00223 |     min |
|        Median refresh time per shard |              |     0.3881 |    0.415467 |  0.02737 |     min |
|           Max refresh time per shard |              |    6.88817 |     7.51323 |  0.62507 |     min |
|                     Total flush time |              |    2.35718 |     2.49198 |   0.1348 |     min |
|             Min flush time per shard |              |    0.00035 |           0 | -0.00035 |     min |
|          Median flush time per shard |              | 0.00336667 |    0.004525 |  0.00116 |     min |
|             Max flush time per shard |              |     0.5248 |    0.493767 | -0.03103 |     min |
|                   Total Young Gen GC |              |    584.708 |     889.803 |  305.095 |       s |
|                     Total Old Gen GC |              |     72.804 |       3.569 |  -69.235 |       s |
|                           Store size |              |    19.0441 |     19.0526 |  0.00851 |      GB |
|                        Translog size |              |    14.6954 |      14.755 |  0.05965 |      GB |
|               Heap used for segments |              |    81.1885 |     81.4526 |  0.26417 |      MB |
|             Heap used for doc values |              |   0.138538 |     0.13168 | -0.00686 |      MB |
|                  Heap used for terms |              |    68.3734 |     68.6373 |  0.26393 |      MB |
|                  Heap used for norms |              |  0.0333252 |   0.0343018 |  0.00098 |      MB |
|                 Heap used for points |              |    5.65936 |      5.6637 |  0.00434 |      MB |
|          Heap used for stored fields |              |    6.98383 |     6.98561 |  0.00178 |      MB |
|                        Segment count |              |        547 |         563 |       16 |         |
|                       Min Throughput | index-append |    42864.4 |     40291.8 | -2572.52 |  docs/s |
|                    Median Throughput | index-append |    45159.7 |     43237.5 | -1922.15 |  docs/s |
|                       Max Throughput | index-append |    46552.9 |     44298.6 | -2254.29 |  docs/s |
|              50th percentile latency | index-append |    820.652 |     859.372 |    38.72 |      ms |
|              90th percentile latency | index-append |    1197.61 |     1239.86 |  42.2422 |      ms |
|              99th percentile latency | index-append |    2156.19 |      3130.3 |  974.108 |      ms |
|            99.9th percentile latency | index-append |     3190.3 |     5339.84 |  2149.54 |      ms |
|           99.99th percentile latency | index-append |     4584.1 |     6758.01 |  2173.91 |      ms |
|             100th percentile latency | index-append |    4680.28 |     7251.15 |  2570.87 |      ms |
|         50th percentile service time | index-append |    820.652 |     859.372 |    38.72 |      ms |
|         90th percentile service time | index-append |    1197.61 |     1239.86 |  42.2422 |      ms |
|         99th percentile service time | index-append |    2156.19 |      3130.3 |  974.108 |      ms |
|       99.9th percentile service time | index-append |     3190.3 |     5339.84 |  2149.54 |      ms |
|      99.99th percentile service time | index-append |     4584.1 |     6758.01 |  2173.91 |      ms |
|        100th percentile service time | index-append |    4680.28 |     7251.15 |  2570.87 |      ms |
|                           error rate | index-append |          0 |           0 |        0 |       % |
|                       Min Throughput |      default |    5.15924 |     5.25423 |  0.09499 |   ops/s |
|                    Median Throughput |      default |    5.21455 |     5.28522 |  0.07066 |   ops/s |
|                       Max Throughput |      default |    5.25937 |     5.31135 |  0.05198 |   ops/s |
|              50th percentile latency |      default |    36831.5 |     35470.1 | -1361.39 |      ms |
|              90th percentile latency |      default |    38662.1 |     37513.1 | -1149.03 |      ms |
|              99th percentile latency |      default |    39053.7 |     37991.9 | -1061.86 |      ms |
|             100th percentile latency |      default |    39107.1 |       38043 | -1064.12 |      ms |
|         50th percentile service time |      default |     169.71 |     177.381 |  7.67012 |      ms |
|         90th percentile service time |      default |    178.218 |     183.915 |  5.69676 |      ms |
|         99th percentile service time |      default |    187.883 |     189.161 |  1.27734 |      ms |
|        100th percentile service time |      default |    195.832 |     195.293 | -0.53852 |      ms |
|                           error rate |      default |          0 |           0 |        0 |       % |
|                       Min Throughput |         term |    26.1242 |     19.0592 | -7.06494 |   ops/s |
|                    Median Throughput |         term |    26.1242 |     19.7042 | -6.41995 |   ops/s |
|                       Max Throughput |         term |    26.1242 |     20.2959 | -5.82833 |   ops/s |
|              50th percentile latency |         term |    10047.6 |       16983 |  6935.31 |      ms |
|              90th percentile latency |         term |    10207.1 |     17267.4 |  7060.35 |      ms |
|              99th percentile latency |         term |    10249.6 |     17316.4 |  7066.81 |      ms |
|             100th percentile latency |         term |    10255.1 |     17322.1 |  7066.97 |      ms |
|         50th percentile service time |         term |     15.303 |     25.6848 |  10.3818 |      ms |
|         90th percentile service time |         term |    16.4165 |     27.7165 |     11.3 |      ms |
|         99th percentile service time |         term |    25.4912 |     54.0499 |  28.5587 |      ms |
|        100th percentile service time |         term |    25.5634 |     59.3632 |  33.7998 |      ms |
|                           error rate |         term |          0 |           0 |        0 |       % |
|                       Min Throughput |        range |   0.725108 |    0.681777 | -0.04333 |   ops/s |
|                    Median Throughput |        range |   0.732718 |    0.688433 | -0.04429 |   ops/s |
|                       Max Throughput |        range |   0.738384 |    0.692403 | -0.04598 |   ops/s |
|              50th percentile latency |        range |     105733 |      118946 |  13212.9 |      ms |
|              90th percentile latency |        range |     131742 |      148894 |    17152 |      ms |
|              99th percentile latency |        range |     137564 |      155508 |  17943.9 |      ms |
|             100th percentile latency |        range |     138195 |      156182 |  17987.3 |      ms |
|         50th percentile service time |        range |    1321.63 |     1415.01 |  93.3769 |      ms |
|         90th percentile service time |        range |    1415.48 |     1498.35 |  82.8698 |      ms |
|         99th percentile service time |        range |    1486.73 |      1544.8 |  58.0686 |      ms |
|        100th percentile service time |        range |    1514.99 |      1602.9 |  87.9114 |      ms |
|                           error rate |        range |          0 |           0 |        0 |       % |
|                       Min Throughput |   hourly_agg |   0.185432 |    0.165594 | -0.01984 |   ops/s |
|                    Median Throughput |   hourly_agg |   0.186429 |    0.166175 | -0.02025 |   ops/s |
|                       Max Throughput |   hourly_agg |   0.187044 |    0.166809 | -0.02024 |   ops/s |
|              50th percentile latency |   hourly_agg |    59814.4 |      158272 |  98457.9 |      ms |
|              90th percentile latency |   hourly_agg |      71797 |      194827 |   123030 |      ms |
|              99th percentile latency |   hourly_agg |    74170.4 |      203039 |   128869 |      ms |
|             100th percentile latency |   hourly_agg |    74640.4 |      203979 |   129338 |      ms |
|         50th percentile service time |   hourly_agg |    5304.58 |     5962.79 |  658.209 |      ms |
|         90th percentile service time |   hourly_agg |    5488.42 |     6161.15 |  672.734 |      ms |
|         99th percentile service time |   hourly_agg |    5636.58 |     6266.38 |  629.805 |      ms |
|        100th percentile service time |   hourly_agg |    6129.61 |     6323.44 |  193.827 |      ms |
|                           error rate |   hourly_agg |          0 |           0 |        0 |       % |
|                       Min Throughput |       scroll |    18.1871 |     17.7633 | -0.42377 | pages/s |
|                    Median Throughput |       scroll |    18.3742 |      17.859 | -0.51527 | pages/s |
|                       Max Throughput |       scroll |     18.416 |     17.9942 | -0.42179 | pages/s |
|              50th percentile latency |       scroll |    73098.5 |     81231.7 |  8133.13 |      ms |
|              90th percentile latency |       scroll |     101527 |      110553 |  9025.89 |      ms |
|              99th percentile latency |       scroll |     107692 |      117077 |  9384.82 |      ms |
|             100th percentile latency |       scroll |     108346 |      117843 |  9497.23 |      ms |
|         50th percentile service time |       scroll |     1347.2 |     1376.18 |  28.9804 |      ms |
|         90th percentile service time |       scroll |    1391.85 |     1438.93 |  47.0797 |      ms |
|         99th percentile service time |       scroll |    1452.59 |     1536.47 |  83.8739 |      ms |
|        100th percentile service time |       scroll |    1475.89 |     1640.97 |  165.087 |      ms |
|                           error rate |       scroll |          0 |           0 |        0 |       % |

```

## Create custom tracks
- benchmark 실행을 위한 데이터를 직접 만들어서 사용한다.
- https://esrally.readthedocs.io/en/latest/adding_tracks.html

### Download benchmark data
```
> mkdir -p ~/rally-tracks/tutorial
> cd ~/rally-tracks/tutorial

# http://www.geonames.org/export/zip/ 참고
> wget https://raw.githubusercontent.com/nicolas-grekas/geonames-data/master/data/allCountries.txt
> sudo yum install -y unzip
> unzip allCountries.zip
> vi toJSON.py
```

- 아래 cols에서 benchmark 테스트에 필요한 필드만 선택적으로 생성할 수 있다.
- "("asciiname", "string", False)" 처럼 False로 설정하면, 테스트 데이터를 생성하지 않음.
```python
import json

cols = (("geonameid", "int", True),
        ("name", "string", True),
        ("asciiname", "string", False),
        ("alternatenames", "string", False),
        ("latitude", "double", True),
        ("longitude", "double", True),
        ("feature_class", "string", False),
        ("feature_code", "string", False),
        ("country_code", "string", True),
        ("cc2", "string", False),
        ("admin1_code", "string", False),
        ("admin2_code", "string", False),
        ("admin3_code", "string", False),
        ("admin4_code", "string", False),
        ("population", "long", True),
        ("elevation", "int", False),
        ("dem", "string", False),
        ("timezone", "string", False))


def main():
    with open("allCountries.txt", "rt", encoding="UTF-8") as f:
        for line in f:
            tup = line.strip().split("\t")
            record = {}
            for i in range(len(cols)):
                name, type, include = cols[i]
                if tup[i] != "" and include:
                    if type in ("int", "long"):
                        record[name] = int(tup[i])
                    elif type == "double":
                        record[name] = float(tup[i])
                    elif type == "string":
                        record[name] = tup[i]
            print(json.dumps(record, ensure_ascii=False))


if __name__ == "__main__":
    main()
```

```
> python36 toJSON.py > documents.json
```

### Crate elasticsearch index mapping file
```
> cd ~/rally-tracks/tutorial
> vi index.json
```

```json
{
  "settings": {
    "index.number_of_replicas": 0
  },
  "mappings": {
    "docs": {
      "dynamic": "strict",
      "properties": {
        "geonameid": {
          "type": "long"
        },
        "name": {
          "type": "text"
        },
        "latitude": {
          "type": "double"
        },
        "longitude": {
          "type": "double"
        },
        "country_code": {
          "type": "text"
        },
        "population": {
          "type": "long"
        }
      }
    }
  }
}
```

### Create Track operation file
```
> cd ~/rally-tracks/tutorial
> vi track.json
```
- 아래 항목 중 document.json 파일에 대한 정보를 미리 입력하면, 실행속도가 향상됨.
  - wc -l documents.json 명령어로 확인 : "document-count": 8221789,
  - stat documents.json 명령어로 확인 : "uncompressed-bytes": 1088282607
- operation-type에 대한 정보는 아래 코드로 확인 가능
  - https://esrally.readthedocs.io/en/stable/track.html
  - https://github.com/elastic/rally/blob/master/esrally/driver/runner.py

#### Track 관련 파라미터들
- https://esrally.readthedocs.io/en/stable/track.html
### Race룰 살행할 때 변경가능한 파라미터 설정
- https://esrally.readthedocs.io/en/stable/command_line_reference.html?highlight=bulk-size
- bulk-size, clients 등의 변수를 실행시점에 정의할 수 있도록 설정
```json
{
  "version": 2,
  "description": "Tutorial benchmark for Rally",
  "indices": [
    {
      "name": "geonames",
      "body": "index.json",
      "types": [ "docs" ]
    }
  ],
  "corpora": [
    {
      "name": "rally-tutorial",
      "documents": [
        {
          "source-file": "documents.json",
          "document-count": 8221789,
          "uncompressed-bytes": 1088282607
        }
      ]
    }
  ],
  "schedule": [
    {
      "operation": {
        "operation-type": "delete-index"
      }
    },
    {
      "operation": {
        "operation-type": "create-index"
      }
    },
    {
      "operation": {
        "operation-type": "cluster-health",
        "request-params": {
          "wait_for_status": "green"
        }
      }
    },
    {
      "operation": {
        "operation-type": "bulk",
        "bulk-size": {{ bulk_size|default(5000) }}
      },
      "warmup-time-period": 10,
      "clients": {{ clients|default(8) }}
    },
    {
      "operation": {
        "operation-type": "force-merge"
      }
    },
    {
      "operation": {
        "name": "query-match-all",
        "operation-type": "search",
        "body": {
          "query": {
            "match_all": {}
          }
        }
      },
      "clients": 8,
      "warmup-iterations": 10,
      "iterations": 1000,
      "target-throughput": 1000
    }
  ]
}
```

### 생성된 track 확인
```
> esrally list tracks --track-path=~/rally-tracks/tutorial
Available tracks:

Name      Description                     Documents  Compressed Size    Uncompressed Size
--------  ----------------------------  -----------  -----------------  -------------------
tutorial  Tutorial benchmark for Rally      8221789  N/A                1.0 GB
```

### 생성된 track 실행
```
> esrally --track-path=~/rally-tracks/tutorial --target-hosts=35.243.157.238:9200 --pipeline=benchmark-only
|   Lap |                               Metric |            Task |      Value |   Unit |
|------:|-------------------------------------:|----------------:|-----------:|-------:|
|   All |                  Total indexing time |                 |     4.0358 |    min |
|   All |          Min indexing time per shard |                 |      0.672 |    min |
|   All |       Median indexing time per shard |                 |   0.832683 |    min |
|   All |          Max indexing time per shard |                 |     0.8748 |    min |
|   All |                     Total merge time |                 |    2.41658 |    min |
|   All |             Min merge time per shard |                 |   0.441567 |    min |
|   All |          Median merge time per shard |                 |   0.486583 |    min |
|   All |             Max merge time per shard |                 |   0.507267 |    min |
|   All |            Total merge throttle time |                 |   0.397867 |    min |
|   All |    Min merge throttle time per shard |                 |    0.06875 |    min |
|   All | Median merge throttle time per shard |                 |  0.0798833 |    min |
|   All |    Max merge throttle time per shard |                 |    0.08605 |    min |
|   All |                   Total refresh time |                 |    1.00588 |    min |
|   All |           Min refresh time per shard |                 |     0.1979 |    min |
|   All |        Median refresh time per shard |                 |   0.201183 |    min |
|   All |           Max refresh time per shard |                 |    0.20425 |    min |
|   All |                   Total Young Gen GC |                 |      4.345 |      s |
|   All |                     Total Old Gen GC |                 |      0.107 |      s |
|   All |                           Store size |                 |    0.88134 |     GB |
|   All |                        Translog size |                 |    1.56223 |     GB |
|   All |               Heap used for segments |                 |     4.1101 |     MB |
|   All |             Heap used for doc values |                 |  0.0239182 |     MB |
|   All |                  Heap used for terms |                 |    3.55904 |     MB |
|   All |                  Heap used for norms |                 | 0.00878906 |     MB |
|   All |                 Heap used for points |                 |   0.262046 |     MB |
|   All |          Heap used for stored fields |                 |   0.256302 |     MB |
|   All |                        Segment count |                 |         72 |        |
|   All |                       Min Throughput |            bulk |    22672.1 | docs/s |
|   All |                    Median Throughput |            bulk |    32948.7 | docs/s |
|   All |                       Max Throughput |            bulk |    37132.7 | docs/s |
|   All |              50th percentile latency |            bulk |    984.178 |     ms |
|   All |              90th percentile latency |            bulk |    1882.07 |     ms |
|   All |              99th percentile latency |            bulk |    2120.75 |     ms |
|   All |             100th percentile latency |            bulk |    2932.55 |     ms |
|   All |         50th percentile service time |            bulk |    984.178 |     ms |
|   All |         90th percentile service time |            bulk |    1882.07 |     ms |
|   All |         99th percentile service time |            bulk |    2120.75 |     ms |
|   All |        100th percentile service time |            bulk |    2932.55 |     ms |
|   All |                           error rate |            bulk |          0 |      % |
|   All |                       Min Throughput | query-match-all |      22.44 |  ops/s |
|   All |                    Median Throughput | query-match-all |      23.06 |  ops/s |
|   All |                       Max Throughput | query-match-all |       23.3 |  ops/s |
|   All |              50th percentile latency | query-match-all |     395742 |     ms |
|   All |              90th percentile latency | query-match-all |     508552 |     ms |
|   All |              99th percentile latency | query-match-all |     533795 |     ms |
|   All |            99.9th percentile latency | query-match-all |     536309 |     ms |
|   All |             100th percentile latency | query-match-all |     536591 |     ms |
|   All |         50th percentile service time | query-match-all |    354.494 |     ms |
|   All |         90th percentile service time | query-match-all |    367.102 |     ms |
|   All |         99th percentile service time | query-match-all |    378.084 |     ms |
|   All |       99.9th percentile service time | query-match-all |     383.55 |     ms |
|   All |        100th percentile service time | query-match-all |    386.664 |     ms |
|   All |                           error rate | query-match-all |          0 |      % |


----------------------------------
[INFO] SUCCESS (took 1105 seconds)
----------------------------------
```

```
> esrally list races
> esrally compare --baseline=20181028T234358Z --contender=20181029T050151Z
|                  Total indexing time |                 |     4.0358 |     4.50627 |  0.47047 |    min |
|          Min indexing time per shard |                 |      0.672 |    0.769833 |  0.09783 |    min |
|       Median indexing time per shard |                 |   0.832683 |    0.923917 |  0.09123 |    min |
|          Max indexing time per shard |                 |     0.8748 |    0.946017 |  0.07122 |    min |
|                     Total merge time |                 |    2.41658 |     2.36282 | -0.05377 |    min |
|             Min merge time per shard |                 |   0.441567 |       0.419 | -0.02257 |    min |
|          Median merge time per shard |                 |   0.486583 |    0.467983 |  -0.0186 |    min |
|             Max merge time per shard |                 |   0.507267 |     0.52145 |  0.01418 |    min |
|            Total merge throttle time |                 |   0.397867 |      0.4056 |  0.00773 |    min |
|    Min merge throttle time per shard |                 |    0.06875 |   0.0593833 | -0.00937 |    min |
| Median merge throttle time per shard |                 |  0.0798833 |   0.0952167 |  0.01533 |    min |
|    Max merge throttle time per shard |                 |    0.08605 |     0.09605 |     0.01 |    min |
|                   Total refresh time |                 |    1.00588 |     1.16557 |  0.15968 |    min |
|           Min refresh time per shard |                 |     0.1979 |    0.227133 |  0.02923 |    min |
|        Median refresh time per shard |                 |   0.201183 |      0.2309 |  0.02972 |    min |
|           Max refresh time per shard |                 |    0.20425 |    0.243583 |  0.03933 |    min |
|                   Total Young Gen GC |                 |      4.345 |      14.679 |   10.334 |      s |
|                     Total Old Gen GC |                 |      0.107 |        0.15 |    0.043 |      s |
|                           Store size |                 |    0.88134 |    0.881283 |   -6e-05 |     GB |
|                        Translog size |                 |    1.56223 |     1.56223 |        0 |     GB |
|               Heap used for segments |                 |     4.1101 |     4.12845 |  0.01836 |     MB |
|             Heap used for doc values |                 |  0.0239182 |   0.0230446 | -0.00087 |     MB |
|                  Heap used for terms |                 |    3.55904 |     3.57436 |  0.01532 |     MB |
|                  Heap used for norms |                 | 0.00878906 |  0.00939941 |  0.00061 |     MB |
|                 Heap used for points |                 |   0.262046 |    0.261709 | -0.00034 |     MB |
|          Heap used for stored fields |                 |   0.256302 |    0.259941 |  0.00364 |     MB |
|                        Segment count |                 |         72 |          77 |        5 |        |
|                       Min Throughput |            bulk |    22672.1 |       11479 | -11193.1 | docs/s |
|                    Median Throughput |            bulk |    32948.7 |     28490.6 | -4458.06 | docs/s |
|                       Max Throughput |            bulk |    37132.7 |     40825.9 |  3693.24 | docs/s |
|              50th percentile latency |            bulk |    984.178 |     960.018 | -24.1598 |     ms |
|              90th percentile latency |            bulk |    1882.07 |     4834.85 |  2952.78 |     ms |
|              99th percentile latency |            bulk |    2120.75 |     5503.24 |   3382.5 |     ms |
|             100th percentile latency |            bulk |    2932.55 |     7263.45 |   4330.9 |     ms |
|         50th percentile service time |            bulk |    984.178 |     960.018 | -24.1598 |     ms |
|         90th percentile service time |            bulk |    1882.07 |     4834.85 |  2952.78 |     ms |
|         99th percentile service time |            bulk |    2120.75 |     5503.24 |   3382.5 |     ms |
|        100th percentile service time |            bulk |    2932.55 |     7263.45 |   4330.9 |     ms |
|                           error rate |            bulk |          0 |           0 |        0 |      % |
|                       Min Throughput | query-match-all |    22.4383 |     30.4841 |  8.04583 |  ops/s |
|                    Median Throughput | query-match-all |    23.0592 |     30.5055 |  7.44625 |  ops/s |
|                       Max Throughput | query-match-all |    23.3049 |     30.5278 |  7.22293 |  ops/s |
|              50th percentile latency | query-match-all |     395742 |      273181 |  -122561 |     ms |
|              90th percentile latency | query-match-all |     508552 |      346182 |  -162370 |     ms |
|              99th percentile latency | query-match-all |     533795 |      362635 |  -171161 |     ms |
|            99.9th percentile latency | query-match-all |     536309 |      364289 |  -172020 |     ms |
|             100th percentile latency | query-match-all |     536591 |      364658 |  -171933 |     ms |
|         50th percentile service time | query-match-all |    354.494 |     262.153 | -92.3407 |     ms |
|         90th percentile service time | query-match-all |    367.102 |     265.686 | -101.416 |     ms |
|         99th percentile service time | query-match-all |    378.084 |     270.871 | -107.213 |     ms |
|       99.9th percentile service time | query-match-all |     383.55 |     288.568 | -94.9813 |     ms |
|        100th percentile service time | query-match-all |    386.664 |     293.627 |  -93.037 |     ms |
|                           error rate | query-match-all |          0 |           0 |        0 |      % |
```

#### Run using track flags (bulk-size=2000)
- bulk-size:2000
- 초당 처리량은 감소  : Median Throughput |   bulk |    28490.6 |     27846.5 | -644.129 | docs/s
- indexing time은 빠름 : Total indexing time |        |    4.50627 |     4.22127 |   -0.285 |    min
```
> esrally --track-path=~/rally-tracks/tutorial --track-params="bulk_size:2000" --target-hosts=35.243.157.238:9200 --pipeline=benchmark-only
|                               Metric |   Task |   Baseline |   Contender |     Diff |   Unit |
|-------------------------------------:|-------:|-----------:|------------:|---------:|-------:|
|                  Total indexing time |        |    4.50627 |     4.22127 |   -0.285 |    min |
|          Min indexing time per shard |        |   0.769833 |     0.73565 | -0.03418 |    min |
|       Median indexing time per shard |        |   0.923917 |        0.86 | -0.06392 |    min |
|          Max indexing time per shard |        |   0.946017 |    0.905167 | -0.04085 |    min |
|                     Total merge time |        |    2.36282 |     2.12658 | -0.23623 |    min |
|             Min merge time per shard |        |      0.419 |    0.403683 | -0.01532 |    min |
|          Median merge time per shard |        |   0.467983 |      0.4241 | -0.04388 |    min |
|             Max merge time per shard |        |    0.52145 |    0.470083 | -0.05137 |    min |
|            Total merge throttle time |        |     0.4056 |    0.405967 |  0.00037 |    min |
|    Min merge throttle time per shard |        |  0.0593833 |   0.0731833 |   0.0138 |    min |
| Median merge throttle time per shard |        |  0.0952167 |   0.0757333 | -0.01948 |    min |
|    Max merge throttle time per shard |        |    0.09605 |    0.100833 |  0.00478 |    min |
|                   Total refresh time |        |    1.16557 |     1.16423 | -0.00133 |    min |
|           Min refresh time per shard |        |   0.227133 |    0.220567 | -0.00657 |    min |
|        Median refresh time per shard |        |     0.2309 |      0.2353 |   0.0044 |    min |
|           Max refresh time per shard |        |   0.243583 |    0.240883 |  -0.0027 |    min |
|                   Total Young Gen GC |        |     14.679 |      12.916 |   -1.763 |      s |
|                     Total Old Gen GC |        |       0.15 |           0 |    -0.15 |      s |
|                           Store size |        |   0.881283 |    0.886505 |  0.00522 |     GB |
|                        Translog size |        |    1.56223 |     1.56223 |        0 |     GB |
|               Heap used for segments |        |    4.12845 |     4.07557 | -0.05289 |     MB |
|             Heap used for doc values |        |  0.0230446 |   0.0260582 |  0.00301 |     MB |
|                  Heap used for terms |        |    3.57436 |     3.51705 | -0.05731 |     MB |
|                  Heap used for norms |        | 0.00939941 |  0.00939941 |        0 |     MB |
|                 Heap used for points |        |   0.261709 |    0.262907 |   0.0012 |     MB |
|          Heap used for stored fields |        |   0.259941 |    0.260155 |  0.00021 |     MB |
|                        Segment count |        |         77 |          77 |        0 |        |
|                       Min Throughput |   bulk |      11479 |     13557.9 |  2078.93 | docs/s |
|                    Median Throughput |   bulk |    28490.6 |     27846.5 | -644.129 | docs/s |
|                       Max Throughput |   bulk |    40825.9 |     35660.8 | -5165.15 | docs/s |
|              50th percentile latency |   bulk |    960.018 |     357.961 | -602.056 |     ms |
|              90th percentile latency |   bulk |    4834.85 |     837.711 | -3997.13 |     ms |
|              99th percentile latency |   bulk |    5503.24 |     1027.43 | -4475.82 |     ms |
|             100th percentile latency |   bulk |    7263.45 |     2828.74 | -4434.71 |     ms |
|         50th percentile service time |   bulk |    960.018 |     357.961 | -602.056 |     ms |
|         90th percentile service time |   bulk |    4834.85 |     837.711 | -3997.13 |     ms |
|         99th percentile service time |   bulk |    5503.24 |     1027.43 | -4475.82 |     ms |
|        100th percentile service time |   bulk |    7263.45 |     2828.74 | -4434.71 |     ms |
|                           error rate |   bulk |          0 |           0 |        0 |      % |
```


#### Run using track flags (bulk-size=10000)
- 초당 처리량은 증가  : Median Throughput |   bulk |    27846.5 |     52795.4 |  24948.9 | docs/s
- indexing time은 늦음 : Total indexing time |        |    4.22127 |     4.73937 |   0.5181 |    min
- 즉, bulk-size를 증가시키면, 처리량은 높아지나 처리속도는 증가(느려짐)
```
> esrally compare --baseline=20181101T102143Z --contender=20181101T103308Z
|                               Metric |   Task |   Baseline |   Contender |     Diff |   Unit |
|-------------------------------------:|-------:|-----------:|------------:|---------:|-------:|
|                  Total indexing time |        |    4.22127 |     4.73937 |   0.5181 |    min |
|          Min indexing time per shard |        |    0.73565 |      0.8558 |  0.12015 |    min |
|       Median indexing time per shard |        |       0.86 |      0.9574 |   0.0974 |    min |
|          Max indexing time per shard |        |   0.905167 |    0.990267 |   0.0851 |    min |
|                     Total merge time |        |    2.12658 |      1.9772 | -0.14938 |    min |
|             Min merge time per shard |        |   0.403683 |    0.387017 | -0.01667 |    min |
|          Median merge time per shard |        |     0.4241 |    0.390917 | -0.03318 |    min |
|             Max merge time per shard |        |   0.470083 |      0.4191 | -0.05098 |    min |
|            Total merge throttle time |        |   0.405967 |    0.465517 |  0.05955 |    min |
|    Min merge throttle time per shard |        |  0.0731833 |   0.0907333 |  0.01755 |    min |
| Median merge throttle time per shard |        |  0.0757333 |   0.0924333 |   0.0167 |    min |
|    Max merge throttle time per shard |        |   0.100833 |   0.0968833 | -0.00395 |    min |
|                   Total refresh time |        |    1.16423 |     1.06843 |  -0.0958 |    min |
|           Min refresh time per shard |        |   0.220567 |    0.192683 | -0.02788 |    min |
|        Median refresh time per shard |        |     0.2353 |    0.211233 | -0.02407 |    min |
|           Max refresh time per shard |        |   0.240883 |    0.232567 | -0.00832 |    min |
|                   Total Young Gen GC |        |     12.916 |      15.681 |    2.765 |      s |
|                     Total Old Gen GC |        |          0 |           0 |        0 |      s |
|                           Store size |        |   0.886505 |    0.865449 | -0.02106 |     GB |
|                        Translog size |        |    1.56223 |     1.56223 |       -0 |     GB |
|               Heap used for segments |        |    4.07557 |      4.4044 |  0.32884 |     MB |
|             Heap used for doc values |        |  0.0260582 |   0.0165672 | -0.00949 |     MB |
|                  Heap used for terms |        |    3.51705 |     3.85654 |   0.3395 |     MB |
|                  Heap used for norms |        | 0.00939941 |  0.00866699 | -0.00073 |     MB |
|                 Heap used for points |        |   0.262907 |    0.258511 |  -0.0044 |     MB |
|          Heap used for stored fields |        |   0.260155 |    0.264114 |  0.00396 |     MB |
|                        Segment count |        |         77 |          71 |       -6 |        |
|                       Min Throughput |   bulk |    13557.9 |       40288 |  26730.1 | docs/s |
|                    Median Throughput |   bulk |    27846.5 |     52795.4 |  24948.9 | docs/s |
|                       Max Throughput |   bulk |    35660.8 |     54917.1 |  19256.4 | docs/s |
|              50th percentile latency |   bulk |    357.961 |     1386.86 |   1028.9 |     ms |
|              90th percentile latency |   bulk |    837.711 |     2025.32 |  1187.61 |     ms |
|              99th percentile latency |   bulk |    1027.43 |     2555.08 |  1527.66 |     ms |
|             100th percentile latency |   bulk |    2828.74 |     5391.67 |  2562.93 |     ms |
|         50th percentile service time |   bulk |    357.961 |     1386.86 |   1028.9 |     ms |
|         90th percentile service time |   bulk |    837.711 |     2025.32 |  1187.61 |     ms |
|         99th percentile service time |   bulk |    1027.43 |     2555.08 |  1527.66 |     ms |
|        100th percentile service time |   bulk |    2828.74 |     5391.67 |  2562.93 |     ms |
|                           error rate |   bulk |          0 |           0 |        0 |      % |
```

#### Run using track flags (bulk-size=10000, elasticsearch mapping 변경-keyword)
- text type을 keyword type으로 변경 (정확한 검색만 가능하도록, 즉 analyzer를 적용하지 않음)
- geonameid, country_code, name 3개 필드만 변경
- 초당 처리량은 감소  : Median Throughput |   bulk |    52795.4 |     46863.5 | -5931.88 | docs/s |
- indexing time은 늦음 : Total indexing time |        |    4.22127 |     4.73937 |   0.5181 |    min
- 일부 필드만 keyword로 변경하는 것은 전반적인 성능 저하 유발
```
{
  "settings": {
    "index.number_of_replicas": 0
  },
  "mappings": {
    "docs": {
      "dynamic": "strict",
      "properties": {
        "geonameid": {
          "type": "keyword"
        },
        "name": {
          "type": "keyword"
        },
        "latitude": {
          "type": "double"
        },
        "longitude": {
          "type": "double"
        },
        "country_code": {
          "type": "keyword"
        },
        "population": {
          "type": "long"
        }
      }
    }
  }
}

|                               Metric |   Task |   Baseline |   Contender |     Diff |   Unit |
|-------------------------------------:|-------:|-----------:|------------:|---------:|-------:|
|                  Total indexing time |        |    4.73937 |     5.24282 |  0.50345 |    min |
|          Min indexing time per shard |        |     0.8558 |    0.943633 |  0.08783 |    min |
|       Median indexing time per shard |        |     0.9574 |     1.05935 |  0.10195 |    min |
|          Max indexing time per shard |        |   0.990267 |     1.11495 |  0.12468 |    min |
|                     Total merge time |        |     1.9772 |     2.52178 |  0.54458 |    min |
|             Min merge time per shard |        |   0.387017 |     0.34165 | -0.04537 |    min |
|          Median merge time per shard |        |   0.390917 |    0.557883 |  0.16697 |    min |
|             Max merge time per shard |        |     0.4191 |    0.638517 |  0.21942 |    min |
|            Total merge throttle time |        |   0.465517 |    0.258983 | -0.20653 |    min |
|    Min merge throttle time per shard |        |  0.0907333 |           0 | -0.09073 |    min |
| Median merge throttle time per shard |        |  0.0924333 |     0.07835 | -0.01408 |    min |
|    Max merge throttle time per shard |        |  0.0968833 |   0.0908833 |   -0.006 |    min |
|                   Total refresh time |        |    1.06843 |     1.56808 |  0.49965 |    min |
|           Min refresh time per shard |        |   0.192683 |    0.296683 |    0.104 |    min |
|        Median refresh time per shard |        |   0.211233 |      0.3169 |  0.10567 |    min |
|           Max refresh time per shard |        |   0.232567 |     0.32775 |  0.09518 |    min |
|                   Total Young Gen GC |        |     15.681 |      17.255 |    1.574 |      s |
|                     Total Old Gen GC |        |          0 |       0.159 |    0.159 |      s |
|                           Store size |        |   0.865449 |     1.03224 |  0.16679 |     GB |
|                        Translog size |        |    1.56223 |     1.56223 |        0 |     GB |
|               Heap used for segments |        |     4.4044 |     6.58325 |  2.17885 |     MB |
|             Heap used for doc values |        |  0.0165672 |   0.0312805 |  0.01471 |     MB |
|                  Heap used for terms |        |    3.85654 |     6.06966 |  2.21312 |     MB |
|                  Heap used for norms |        | 0.00866699 |           0 | -0.00867 |     MB |
|                 Heap used for points |        |   0.258511 |    0.217598 | -0.04091 |     MB |
|          Heap used for stored fields |        |   0.264114 |    0.264717 |   0.0006 |     MB |
|                        Segment count |        |         71 |          84 |       13 |        |
|                       Min Throughput |   bulk |      40288 |       35557 | -4731.05 | docs/s |
|                    Median Throughput |   bulk |    52795.4 |     46863.5 | -5931.88 | docs/s |
|                       Max Throughput |   bulk |    54917.1 |     48523.9 | -6393.23 | docs/s |
|              50th percentile latency |   bulk |    1386.86 |     1489.86 |  103.001 |     ms |
|              90th percentile latency |   bulk |    2025.32 |     2286.53 |  261.206 |     ms |
|              99th percentile latency |   bulk |    2555.08 |     2945.26 |  390.174 |     ms |
|             100th percentile latency |   bulk |    5391.67 |     3691.69 | -1699.98 |     ms |
|         50th percentile service time |   bulk |    1386.86 |     1489.86 |  103.001 |     ms |
|         90th percentile service time |   bulk |    2025.32 |     2286.53 |  261.206 |     ms |
|         99th percentile service time |   bulk |    2555.08 |     2945.26 |  390.174 |     ms |
|        100th percentile service time |   bulk |    5391.67 |     3691.69 | -1699.98 |     ms |
|                           error rate |   bulk |          0 |           0 |        0 |      % |
```



### Run using track flags (bulk-size=10000, elasticsearch mapping 변경-keyword)
- text type을 keyword type으로 변경 (정확한 검색만 가능하도록, 즉 analyzer를 적용하지 않음)
- geonameid, country_code, name, latitude, longitude 5개 필드 변경
- 초당 처리량은 감소 : Median Throughput |   bulk |    52795.4 |     46863.5 | -5931.88 | docs/s |
- indexing time은 늦음 : Total indexing time |        |    4.22127 |     4.73937 |   0.5181 |    min
- 일부 필드만 keyword로 변경하는 것은 전반적인 성능 저하 유발
- keyword은 throughtput에 큰 영향을 주지는 못하는 것으로 판단됨.
- 비교대상은 원래 mapping으로 실행한 테스트임

|                               Metric |   Task |   Baseline |   Contender |     Diff |   Unit |
|-------------------------------------:|-------:|-----------:|------------:|---------:|-------:|
|                  Total indexing time |        |    4.73937 |     6.09827 |   1.3589 |    min |
|          Min indexing time per shard |        |     0.8558 |      1.1572 |   0.3014 |    min |
|       Median indexing time per shard |        |     0.9574 |     1.23375 |  0.27635 |    min |
|          Max indexing time per shard |        |   0.990267 |     1.25427 |    0.264 |    min |
|                     Total merge time |        |     1.9772 |     2.02342 |  0.04622 |    min |
|             Min merge time per shard |        |   0.387017 |    0.344533 | -0.04248 |    min |
|          Median merge time per shard |        |   0.390917 |    0.410983 |  0.02007 |    min |
|             Max merge time per shard |        |     0.4191 |    0.434267 |  0.01517 |    min |
|            Total merge throttle time |        |   0.465517 |    0.143167 | -0.32235 |    min |
|    Min merge throttle time per shard |        |  0.0907333 |   0.0208333 |  -0.0699 |    min |
| Median merge throttle time per shard |        |  0.0924333 |   0.0306667 | -0.06177 |    min |
|    Max merge throttle time per shard |        |  0.0968833 |   0.0342333 | -0.06265 |    min |
|                   Total refresh time |        |    1.06843 |     2.24472 |  1.17628 |    min |
|           Min refresh time per shard |        |   0.192683 |    0.441183 |   0.2485 |    min |
|        Median refresh time per shard |        |   0.211233 |    0.451733 |   0.2405 |    min |
|           Max refresh time per shard |        |   0.232567 |    0.453933 |  0.22137 |    min |
|                   Total Young Gen GC |        |     15.681 |      20.078 |    4.397 |      s |
|                     Total Old Gen GC |        |          0 |       0.119 |    0.119 |      s |
|                           Store size |        |   0.865449 |    0.976526 |  0.11108 |     GB |
|                        Translog size |        |    1.56223 |     1.56223 |        0 |     GB |
|               Heap used for segments |        |     4.4044 |     9.17976 |  4.77536 |     MB |
|             Heap used for doc values |        |  0.0165672 |   0.0198517 |  0.00328 |     MB |
|                  Heap used for terms |        |    3.85654 |     8.83616 |  4.97962 |     MB |
|                  Heap used for norms |        | 0.00866699 |           0 | -0.00867 |     MB |
|                 Heap used for points |        |   0.258511 |    0.066123 | -0.19239 |     MB |
|          Heap used for stored fields |        |   0.264114 |    0.257622 | -0.00649 |     MB |
|                        Segment count |        |         71 |          66 |       -5 |        |
|                       Min Throughput |   bulk |      40288 |     32382.1 |  -7905.9 | docs/s |
|                    Median Throughput |   bulk |    52795.4 |     47215.7 | -5579.67 | docs/s |
|                       Max Throughput |   bulk |    54917.1 |     50958.1 | -3959.03 | docs/s |
|              50th percentile latency |   bulk |    1386.86 |     1506.87 |  120.009 |     ms |
|              90th percentile latency |   bulk |    2025.32 |     2230.72 |    205.4 |     ms |
|              99th percentile latency |   bulk |    2555.08 |     3163.89 |  608.803 |     ms |
|             100th percentile latency |   bulk |    5391.67 |     3914.72 | -1476.95 |     ms |
|         50th percentile service time |   bulk |    1386.86 |     1506.87 |  120.009 |     ms |
|         90th percentile service time |   bulk |    2025.32 |     2230.72 |    205.4 |     ms |
|         99th percentile service time |   bulk |    2555.08 |     3163.89 |  608.803 |     ms |
|        100th percentile service time |   bulk |    5391.67 |     3914.72 | -1476.95 |     ms |
|                           error rate |   bulk |          0 |           0 |        0 |      % |


### Run using track flags (bulk-size=15000
- bulk-size를 너무 많이 늘려도, 처리량은 오히려 감소함.
|                               Metric |   Task |   Baseline |   Contender |     Diff |   Unit |
|-------------------------------------:|-------:|-----------:|------------:|---------:|-------:|
|                  Total indexing time |        |    4.73937 |     5.00317 |   0.2638 |    min |
|          Min indexing time per shard |        |     0.8558 |    0.868233 |  0.01243 |    min |
|       Median indexing time per shard |        |     0.9574 |     1.03472 |  0.07732 |    min |
|          Max indexing time per shard |        |   0.990267 |     1.05608 |  0.06582 |    min |
|                     Total merge time |        |     1.9772 |     1.89148 | -0.08572 |    min |
|             Min merge time per shard |        |   0.387017 |      0.3571 | -0.02992 |    min |
|          Median merge time per shard |        |   0.390917 |    0.373367 | -0.01755 |    min |
|             Max merge time per shard |        |     0.4191 |     0.40365 | -0.01545 |    min |
|            Total merge throttle time |        |   0.465517 |    0.422417 |  -0.0431 |    min |
|    Min merge throttle time per shard |        |  0.0907333 |   0.0738333 |  -0.0169 |    min |
| Median merge throttle time per shard |        |  0.0924333 |     0.08595 | -0.00648 |    min |
|    Max merge throttle time per shard |        |  0.0968833 |   0.0929333 | -0.00395 |    min |
|                   Total refresh time |        |    1.06843 |     1.05527 | -0.01317 |    min |
|           Min refresh time per shard |        |   0.192683 |    0.200633 |  0.00795 |    min |
|        Median refresh time per shard |        |   0.211233 |    0.209817 | -0.00142 |    min |
|           Max refresh time per shard |        |   0.232567 |    0.220683 | -0.01188 |    min |
|                   Total Young Gen GC |        |     15.681 |      16.956 |    1.275 |      s |
|                     Total Old Gen GC |        |          0 |       0.075 |    0.075 |      s |
|                           Store size |        |   0.865449 |    0.871192 |  0.00574 |     GB |
|                        Translog size |        |    1.56223 |     1.56223 |        0 |     GB |
|               Heap used for segments |        |     4.4044 |     4.69997 |  0.29557 |     MB |
|             Heap used for doc values |        |  0.0165672 |   0.0309334 |  0.01437 |     MB |
|                  Heap used for terms |        |    3.85654 |     4.13885 |  0.28231 |     MB |
|                  Heap used for norms |        | 0.00866699 |  0.00720215 | -0.00146 |     MB |
|                 Heap used for points |        |   0.258511 |    0.259687 |  0.00118 |     MB |
|          Heap used for stored fields |        |   0.264114 |     0.26329 | -0.00082 |     MB |
|                        Segment count |        |         71 |          59 |      -12 |        |
|                       Min Throughput |   bulk |      40288 |     26827.1 | -13460.9 | docs/s |
|                    Median Throughput |   bulk |    52795.4 |     49179.4 |    -3616 | docs/s |
|                       Max Throughput |   bulk |    54917.1 |     50508.2 | -4408.89 | docs/s |
|              50th percentile latency |   bulk |    1386.86 |     2270.33 |  883.474 |     ms |
|              90th percentile latency |   bulk |    2025.32 |     3563.75 |  1538.43 |     ms |
|              99th percentile latency |   bulk |    2555.08 |     5172.62 |  2617.54 |     ms |
|             100th percentile latency |   bulk |    5391.67 |     7516.95 |  2125.28 |     ms |
|         50th percentile service time |   bulk |    1386.86 |     2270.33 |  883.474 |     ms |
|         90th percentile service time |   bulk |    2025.32 |     3563.75 |  1538.43 |     ms |
|         99th percentile service time |   bulk |    2555.08 |     5172.62 |  2617.54 |     ms |
|        100th percentile service time |   bulk |    5391.67 |     7516.95 |  2125.28 |     ms |
|                           error rate |   bulk |          0 |           0 |        0 |      % |
